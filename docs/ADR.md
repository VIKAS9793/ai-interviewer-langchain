# üìã Architecture Decision Records (ADR)

> **Last Updated:** 2025-12-08

## 1. Cloud-First Deployment

**Status:** Accepted

**Context:** The project was originally designed for local execution with Ollama. However, HuggingFace Spaces provides free hosting with serverless GPU inference.

**Decision:** Adopt a **cloud-first architecture** using HuggingFace Inference API as the primary deployment target.

**Consequences:**
- ‚úÖ No local GPU required
- ‚úÖ Free hosting on HuggingFace Spaces
- ‚úÖ Automatic scaling
- ‚ö†Ô∏è Requires internet connectivity
- ‚ö†Ô∏è Subject to API rate limits

---

## 2. Single-Model Evaluation Strategy (Architecture Simplified)
**Status:** Accepted (Replaces Dual-Model)
**Context:** Multi-model inference (Mistral/Qwen) on Free Tier caused 500 errors ("Task not supported").
**Decision:** Standardize on **Meta-LLaMA-3 (8B)** for BOTH generation and evaluation.
**Consequences:**
- ‚úÖ 100% API Stability (No "Task not supported" errors)
- ‚úÖ Simplified Architecture (DRY)
- ‚ö†Ô∏è Sacrifices nuanced scoring of larger models for reliability

---

## 3. 1-5 Scale Instead of 1-10

**Status:** Accepted

**Context:** Research (Prometheus, FARE) shows that 1-5 scales yield more reliable and consistent LLM evaluations.

**Decision:** Use **1-5 scoring scale** internally, convert to 1-10 for UI display.

**Consequences:**
- ‚úÖ More consistent scores
- ‚úÖ Easier rubric definitions
- ‚úÖ Higher human correlation

---

## 4. Semantic Relevance Checking

**Status:** Accepted

**Context:** The AI was incorrectly scoring off-topic answers highly because heuristics only checked keywords.

**Decision:** Add **embedding-based semantic similarity** using Sentence Transformers.

**Consequences:**
- ‚úÖ Detects off-topic answers accurately
- ‚úÖ Cached embeddings for performance
- ‚ö†Ô∏è Additional dependency (sentence-transformers)
